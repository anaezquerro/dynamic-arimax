---
title: "Overview"
author: "Ana Ezquerro"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    number_sections: yes
    latex_engine: lualatex
    fig_caption: yes
    toc: yes
    highlight: tango
    df_print: kable
    citation_package: biblatex
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    number_sections: true
    theme: paper
    highlight: tango
  prettydoc::html_pretty:
    toc: true
    df_print: paged
    number_sections: true
    theme: hpstr
    highlight: github
fontsize: 12pt
geometry: margin=0.7in
classoption: a4paper
documentclass: article
header-includes:
- \usepackage{sfmath}
- \renewcommand*\familydefault{\sfdefault}
- \renewcommand{\baselinestretch}{1.2}
- \setlength{\parskip}{1em}
- \usepackage{xcolor}
- \input{confi.tex}
always_allow_html: yes
bibliography: references.bib
biblio-style: bwl-FU
linkcitations: true
linkcolor: blue
---

```{r, echo=F, warning=F, message=F}
knitr::opts_chunk$set(fig.align='center', fig.width = 10, 
                      fig.height = 8, message=F, comment='', warning=F)
eval(parse("../plot-tools.R", encoding="UTF-8"))
eval(parse("../auto-fit.R", encoding="UTF-8"))
eval(parse("../auto-select.R", encoding="UTF-8"))
eval(parse("../forecasting.R", encoding="UTF-8"))
eval(parse('../arima-simulation.R', encoding='UTF-8'))

library(fpp2)
library(tseries)
library(TSA)
library(seastests)
library(forecast)

library(plotly)
library(forecast)

library(prettydoc)
library(stringi)
library(stringr)
library(polynom)
library(parallel)

display <- function(fig, name, width=800, height=400) {
  if (is.null(knitr::opts_knit$get("rmarkdown.pandoc.to"))) {
    return(fig)
  }
  if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "latex") {
    figpath <- paste0('figures/', name, ".pdf")
    save_image(fig, figpath, width=width, height=height)
    return(knitr::include_graphics(figpath))
  }
  if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
    fig <- fig %>% layout(width=840, height=700)
    return(fig)
  }
}

dir.create("figures", showWarnings=F)
```

\vspace{2em}

In this notebook we show multiple simulations for our automatic covariates selection method in multiple scenarios with different lags. Diverse examples are explored and our method is tested against a set of covariates where only some of them influence in the target variable with a concrete lag (always lower of equal to 0). In the following sections we propose different scenarios with incremental complexity in order to study the behavior of our proposal in simulated data and infer the performance in real environments.

We used `arima_simulation.R` functions to randomly generate time series from an ARIMA process.

\newpage

# Simulation of a dynamic regression model with stationary errors

In this section we show how our automatic selectio method works on basic examples where modeling errors are stationary:


$$Y_t = \beta_0 + \beta_1 X_{t-r_1}^{(1)} + \beta_2 X_{t-r_2}^{(2)} + \cdots + X_{t-r_p}^{(p)}+ \eta_t, \qquad \eta_t \sim \text{ARMA(p,q)}, \quad r_i \geq 0 \text{ para } i=1,..., p $$


## Model with null lags {#example1}

Assume a dynamic regression model with three regressor variables with null lags (all lags are equal to zero) following:

\begin{equation}\label{eq:ejemplo1}
    Y_t = \beta_0 + \beta_1 X_t^{(1)} + \beta_2 X_t^{(2)} + \beta_3 X_t^{(3)} + \eta_t
\end{equation}

where:

-   $\eta_t \sim$ ARMA(2,1), thus, errors are stationary.
-   $X_t^{(1)} \sim$ ARIMA(2, 1, 3) and its coefficient $\beta_1 = 2.8$.
-   $X_t^{(2)} \sim$ ARIMA(1, 1, 2) and its coefficient $\beta_2 = -1.12$.
-   $X_t^{(3)} \sim$ ARMA(1, 2) and its coefficient $\beta_3 = -2.3$.
-   The *intercept* is $\beta_0=0.8$.

Assume another set of variables (all following an ARIMA process) which do not influence in the target variable:

-   $X_t^{(4)} \sim$ ARIMA(1, 0, 3).
-   $X_t^{(5)} \sim$ ARIMA(2, 1, 2).
-   $X_t^{(6)} \sim$ ARIMA(2, 1, 1).

```{r}
# ---- Generate all variables of the scenario ----
set.seed(12)
N <- 1000

# residuals ~ ARIMA(2,0,1)
residuals <- sim.arima(model=list(p=2, d=0, q=1), n=N, with.constant=FALSE)

# X1 ~ ARIMA(2,1,3)
X1 <- sim.arima(model=list(p=2, d=1, q=3), n=N, with.constant=FALSE)

# X2 ~ ARIMA(1,1,2)
X2 <- sim.arima(model=list(p=1, d=1, q=2), n=N, with.constant=FALSE)

# X3 ~ ARIMA(1,0,2)
X3 <- sim.arima(model=list(p=1, d=0, q=2), n=N, with.constant=FALSE)

# X4 ~ ARIMA(1,0,3)
X4 <- sim.arima(model=list(p=1, d=0, q=3), n=N, with.constant=FALSE)

# X5 ~ ARIMA(2,1,2)
X5 <- sim.arima(model=list(p=2, d=1, q=2), n=N, with.constant=FALSE)

# X6 ~ ARIMA(2,1,1)
X6 <- sim.arima(model=list(p=2, d=1, q=1), n=N, with.constant=FALSE)
```

**Covariates selection and model fitting**: We create the target variable and test the final result of the selection function `drm.select()`:


```{r}
beta0 <- 0.8; beta1 <- 2.8; beta2 <- -1.12; beta3 <- -2.3
Y <- beta0 + beta1 * X1$X + beta2 * X2$X + beta3 * X3$X + residuals$X
regressors <- cbind(X1=X1$X, X2=X2$X, X3=X3$X, X4=X4$X, X5=X5$X, X6=X6$X)
model <- drm.select(Y, regressors, show_info=T)
```

In the output of our method we see that:

1. The selected covariates are $X_t^{(1)}$, $X_t^{(2)}$ and $X_t^{(3)}$.
2. Residuals are stationary and modeled by an ARIMA(2,0,1).
3. Regression coefficients haven been correctly estimated.

**Prediction**: We can estimate point predictions:

```{r}
preds <- forecast_model(Y, regressors, model, h=10, mode='bootstrap')
display(plot_forecast(preds, rang=c(N-50, N+10)), name='example1')
```


## Model where $r_i\geq 0$ para $i=1,...,p$ {#example2}

Let's assume a a dynamic regression model similar to our [first example](#example1) using the same variables but applying a negative lag.


\begin{equation}\label{eq:example2}
    Y_t = \beta_0 + \beta_1 X_{t-r_1}^{(1)} + \beta_2 X_{t-r_2}^{(2)} + \beta_3 X_{t-r_3}^{(3)} + \eta_t
\end{equation}

where:

-   $\eta_t \sim$ ARMA(2,1).
-   $X_t^{(1)} \sim$ ARIMA(2, 1, 3) and its lag $r_1=2$.
-   $X_t^{(2)} \sim$ ARMA(1, 1, 2) and its lag $r_2=0$.
-   $X_t^{(3)} \sim$ ARMA(1, 0, 2) and its lag $r_3=3$.


```{r}
beta0 <- -0.6; beta1 <- 1.7; beta2 <- -2.2; beta3 <- 1.3
r1 <- 2; r3 <- 3
Y <- beta0 + beta1 * lag(X1$X, -r1) + beta2 * X2$X + beta3 * lag(X3$X, -r3) + 
    residuals$X
```


**Covariates selection and model fitting**:

```{r}
regressors <- cbind(X1=X1$X, X2=X2$X, X3=X3$X, X4=X4$X, X5=X5$X, X6=X6$X)
model <- drm.select(Y, regressors, show_info=T, st_method='adf.test')
```
In the output of our function we see our method's selection:

1. The fitted model has stationary residuals generated by an ARIMA(2,0,2).
2. The selected covariates are the same used in the generation of $Y$.
3. The ARIMA process for the residuals do not match with the original orders (we used an ARIMA(2,0,1))
4. Regressor coefficients have been correctly approximated.


**Point predictions**:


```{r, collapse=TRUE}
preds <- forecast_model(Y, regressors, model, h=10, mode='bootstrap')
display(plot_forecast(preds, rang=c(N-50, N+10)), name='example2')
```



# Simulation of a dynamic regression model with ARIMA errors ($d\geq 1$)

In this section we consider dynamic regression models with non-stationary errors:


$$Y_t = \beta_0 + \beta_1 X_{t-r_1}^{(1)} + \beta_2 X_{t-r_2}^{(2)} + \cdots + X_{t-r_p}^{(p)}+ \eta_t, \qquad \eta_t\sim \text{ARIMA(p,d,q)} $$


## Model where $r_i=0$ para $i=1,...,p$ {#example3}

We took the same variables of [our first example](#example1) but adding non-stationary residuals:

\begin{equation}\label{eq:ejemplo3}
     Y_t = \beta_0 + \beta_1 X_t^{(1)} + \beta_2 X_t^{(2)} + \beta_3 X_t^{(3)} + \eta_t, \qquad \eta_t\sim\text{ARIMA(1,2,2)}
\end{equation}

where:

-   $X_t^{(1)} \sim$ ARIMA(2, 1, 3) and its coefficient $\beta_1 = -1.3$.
-   $X_t^{(2)} \sim$ ARIMA(1, 1, 2) and its coefficient $\beta_2 = 2.12$.
-   $X_t^{(3)} \sim$ ARMA(1, 2) and its coefficient $\beta_3 = 2.3$.
-   The *intercept* is $\beta_0=0.8$.

Covariates which do not influence in $Y$ are the same as those proposed in the [first scenario](#example1).

```{r}
set.seed(123)
residuals <- sim.arima(model=list(p=3, d=2, q=2), n=N)

beta0 <- 0.8; beta1 <- -1.3; beta2 <- 1.12; beta3 <- 1.3
Y <- beta0 + beta1 * X1$X + beta2 * X2$X + beta3 * X3$X + 1.1*residuals$X
```

**Covariates selection and model fitting**: We fit the model with original covariates:

```{r}
regressors <- cbind(X1=X1$X, X2=X2$X, X3=X3$X, X4=X4$X, X5=X5$X, X6=X6$X)
model <- drm.select(Y, regressors, show_info=T) 
```


By observing the console output, note that our selection method has applied a regular differentiation to fit an ARIMAX model with stationary errors and has correctly selected the 3 regressor variables which originally influence in the target variable with the correct lags.

**Prediction**: When using a fitted model with differentiation, the function `forecast_model()` raises a warning about using the original scale of data to compute point predictions.


```{r, collapse=TRUE}
preds <- forecast_model(Y, regressors, model, h=10, mode='bootstrap')
display(plot_forecast(preds, rang=c(N-50, N+10)), name='example3')
```



## Model where $r_i \geq 0$ para $i=1,...,p$

We modify the [last example](#example4) to make regressors to influence in $Y$ with a positive lag.


- $X_t^{(1)}$ is introduced with lag $r_1=2$.
- $X_t^{(3)}$ is introduce with lag $r_3=1$.


```{r}
beta0 <- 0.8; beta1 <- -1.3; beta2 <- 2.12; beta3 <- 2.3
r1 <- 2; r3 <- 1
Y <- beta0 + beta1 * lag(X1$X, -r1) + beta2 * X2$X + beta3 * lag(X3$X, -r3) + 1.5*residuals$X
```


**Covariates selection and model fitting**: 

```{r}
regressors <- cbind(X1=X1$X, X2=X2$X, X3=X3$X, X4=X4$X, X5=X5$X, X6=X6$X)
model <- drm.select(Y, regressors, show_info=T, st_method='adf.test')
```

The output is similar to the result of the [last example](#example3). Covariate lags have been correctly selected and the method has applied a regular differentiation to data to fit an ARIMAX model with stationary errors.

\newpage

# Prewhitening method

## With stationary errors

```{r}
set.seed(123)
residuals <- sim.arima(model=list(p=2, d=0, q=2), n=N)

beta0 <- -0.1; beta1 <- 3.2; beta2 <- -2.5
r1 <- 2; r2 <- 3
Y <- beta0 + beta1 * lag(X1$X, -r1) + beta2 * lag(X2$X, -r2) + residuals$X
regressors <- cbind(X1=X1$X, X2=X2$X, X3=X3$X, X4=X4$X, X5=X5$X, X6=X6$X)
```

Fit a model with the stationary detection method `auto.arima`:

```{r}
model <- drm.select(Y, regressors, show_info=T, st_method='auto.arima')
```

Fir a model with the stationary detection method `adf.test`:

```{r}
model <- drm.select(Y, regressors, show_info=T, st_method='adf.test')
```

## With non-stationary errors

```{r}
set.seed(123)
residuals <- sim.arima(model=list(p=2, d=2, q=1), n=N)


beta0 <- -0.1; beta1 <- 3.2; beta2 <- -2.5
r1 <- 2; r2 <- 3
Y <- beta0 + beta1 * lag(X1$X, -r1) + beta2 * lag(X2$X, -r2) + residuals$X
regressors <- cbind(X1=X1$X, X2=X2$X, X3=X3$X, X4=X4$X, X5=X5$X, X6=X6$X)
```

Fit a model with the stationary detection method `auto.arima`:

```{r}
model <- drm.select(Y, regressors, show_info=T, st_method='auto.arima')
```

Fit a model with the stationary detection method `adf.test`:


```{r}
model <- drm.select(Y, regressors, show_info=T, st_method='adf.test')
```
